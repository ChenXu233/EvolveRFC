"""å…±äº«è¾©è®ºé€»è¾‘

å·¥ä½œæµå’Œå¤œé—´å®ˆæŠ¤è¿›ç¨‹å…±ç”¨çš„å¤šè½®è¾©è®ºæœºåˆ¶ã€‚
"""

from typing import Optional, Union, TYPE_CHECKING, Callable, Any, List
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, SystemMessage

from ..agents import get_role_prompt, get_reviewer_roles
from ..settings import get_role_llm_config, BaseLLMConfig
from ..core.state import Viewpoint, ViewpointStatus

# å¯¼å…¥å·¥å…·ï¼ˆç”¨äºå¤šæ®µæ€è€ƒï¼‰
from .tools import get_all_tools

if TYPE_CHECKING:
    pass


def _create_llm_client(
    role_name: str, config: BaseLLMConfig
) -> ChatOpenAI | ChatAnthropic:
    """æ ¹æ®é…ç½®åˆ›å»º LLM å®¢æˆ·ç«¯"""

    if not config.api_key:
        raise ValueError(f"è§’è‰² {role_name} çš„ LLM é…ç½®ç¼ºå°‘ API å¯†é’¥")

    if config.provider == "openai":
        return ChatOpenAI(
            model=config.model,
            temperature=config.temperature,
            base_url=config.base_url,
            api_key=config.api_key,
        )
    elif config.provider == "anthropic":
        return ChatAnthropic(
            model_name=config.model,
            temperature=config.temperature,
            base_url=config.base_url,
            timeout=config.timeout,
            stop=config.stop,
            api_key=config.api_key,
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ provider: {config.provider}")


def _get_client_for_role(role: str) -> ChatOpenAI | ChatAnthropic:
    """è·å–è§’è‰²å¯¹åº”çš„ LLM å®¢æˆ·ç«¯"""
    config = get_role_llm_config(role)
    return _create_llm_client(role, config)


def _format_chunk_content(chunk: Any) -> str:
    """æ ¼å¼åŒ– chunk content ä¸ºå­—ç¬¦ä¸²"""
    content = chunk.content
    if isinstance(content, str):
        return content
    elif isinstance(content, list):
        # å¤„ç†åˆ—è¡¨ç±»å‹çš„ contentï¼ˆå¦‚ image_url ç­‰ï¼‰
        return str(content)
    else:
        return str(content)


def _get_token_usage(
    client: ChatOpenAI | ChatAnthropic,
    system_prompt: str,
    input_text: str,
    full_response: str,
) -> tuple[int, int]:
    """è·å–çœŸå®çš„ token ä½¿ç”¨é‡
    
    å¯¹äºæµå¼è°ƒç”¨ï¼ŒLangChain ä¸ä¼šç«‹å³è¿”å› usageã€‚
    æˆ‘ä»¬é€šè¿‡ invoke æ–¹å¼è·å–å‡†ç¡®å€¼ï¼ˆç”¨äº OpenAI/Anthropicï¼‰ã€‚
    """
    # æ–¹æ³•1ï¼šå°è¯•é€šè¿‡ invoke è·å– usageï¼ˆæœ€å‡†ç¡®ï¼‰
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=input_text),
    ]
    
    try:
        # ä½¿ç”¨éæµå¼è°ƒç”¨è·å–å‡†ç¡®çš„ usage
        response = client.invoke(messages)
        # ä½¿ç”¨ getattr å®‰å…¨è·å– usageï¼ˆé¿å…ç±»å‹æ£€æŸ¥é”™è¯¯ï¼‰
        usage = getattr(response, 'usage', None)
        if usage:
            prompt_tokens = getattr(usage, 'prompt_tokens', None) or getattr(usage, 'input_tokens', None)
            completion_tokens = getattr(usage, 'completion_tokens', None) or getattr(usage, 'output_tokens', None)
            if prompt_tokens is not None and completion_tokens is not None:
                return prompt_tokens, completion_tokens
    except Exception:
        pass
    
    # å›é€€åˆ°ä¼°ç®—
    input_text_length = len(system_prompt) + len(input_text)
    input_tokens = input_text_length // 4
    output_tokens = len(full_response) // 4
    
    return input_tokens, output_tokens


def _run_role_review_stream(
    role: str,
    content: str,
    current_round: int,
    stream_callback: Optional[Callable[[str], None]] = None,
    previous_results: Optional[list[dict]] = None,
    token_callback: Optional[Callable[[dict], None]] = None,
) -> str:
    """æµå¼è¿è¡Œå•ä¸ªè§’è‰²çš„è¯„å®¡

    Args:
        role: è§’è‰²åç§°
        content: å¾…è¯„å®¡å†…å®¹
        current_round: å½“å‰è½®æ¬¡
        stream_callback: æµå¼å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶å†…å®¹ç‰‡æ®µ
        previous_results: ä¹‹å‰è§’è‰²çš„è¯„å®¡ç»“æœï¼Œç”¨äºè¾©è®ºå‚è€ƒ
        token_callback: Tokenä½¿ç”¨é‡å›è°ƒï¼Œæ¥æ”¶ {input_tokens, output_tokens, total_tokens, remaining}

    Returns:
        å®Œæ•´çš„è¯„å®¡å†…å®¹
    """
    import re

    system_prompt = get_role_prompt(role)
    client = _get_client_for_role(role)

    # è·å–æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£å¤§å°ï¼ˆé»˜è®¤ 128Kï¼‰
    config = get_role_llm_config(role)
    model_max_tokens = getattr(config, 'max_tokens', 128000) or 128000

    # æ„å»ºè¾“å…¥æ–‡æœ¬
    input_text = f"è¯·è¯„å®¡ä»¥ä¸‹å†…å®¹ï¼ˆè½®æ¬¡ï¼š{current_round}ï¼‰ï¼š\n\n{content}\n"

    # å¦‚æœæœ‰ä¹‹å‰çš„è¯„å®¡ç»“æœï¼Œæ·»åŠ è¾©è®ºå†å²
    if previous_results:
        input_text += "\n=== ä¹‹å‰è§’è‰²çš„è§‚ç‚¹ ===\n"
        for result in previous_results:
            role_name = result.get("role", "æœªçŸ¥")
            role_content = result.get("content", "")
            role_vote = result.get("vote", "")

            # æå–è®ºç‚¹åˆ—è¡¨
            points = []
            point_pattern = r"è®ºç‚¹\d+[:ï¼š]([^\n]+)"
            for match in re.finditer(point_pattern, role_content):
                points.append(match.group(1).strip())

            input_text += f"\nã€{role_name}ã€‘ç«‹åœº: {role_vote or 'æœªçŸ¥'}\n"
            if points:
                for i, p in enumerate(points[:3], 1):  # åªå–å‰3ä¸ªè®ºç‚¹
                    input_text += f"  è®ºç‚¹{i}: {p[:100]}...\n"
            else:
                input_text += f"  è§‚ç‚¹: {role_content[:200]}...\n"

        input_text += "\n=== ä½ çš„ä»»åŠ¡ ===\nè¯·å‚è€ƒä»¥ä¸Šè§‚ç‚¹è¿›è¡Œè¾©è®ºï¼š\n1. å¦‚æœåŒæ„æŸä¸ªè®ºç‚¹ï¼Œè¡¥å……æ–°çš„è®ºæ®\n2. å¦‚æœåå¯¹æŸä¸ªè®ºç‚¹ï¼Œè¯´æ˜ç†ç”±å¹¶æå‡ºæ›¿ä»£æ–¹æ¡ˆ\n3. å¦‚æœæœ‰æ–°çš„å…³æ³¨ç‚¹ï¼Œç‹¬ç«‹æå‡ºæ–°è®ºç‚¹\n"

    input_text += '\nè¯·ä»ä½ çš„ä¸“ä¸šè§’åº¦è¿›è¡Œè¯„å®¡ï¼Œè¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå¾ªï¼‰ï¼š\n\n## è‚¯å®šç‚¹\n- [å¦‚æœæœ‰å€¼å¾—è‚¯å®šçš„è®¾è®¡ï¼Œå†™åœ¨è¿™é‡Œ]\n\n## è®ºç‚¹åˆ—è¡¨\nè®ºç‚¹1: "<ä¸€å¥è¯æ ¸å¿ƒè§‚ç‚¹>"\nè®ºæ®: ["<æ”¯æ’‘è®ºæ®1>", "<æ”¯æ’‘è®ºæ®2>"]\nç«‹åœº: "èµæˆ|åå¯¹|å¼ƒæƒ"\nç½®ä¿¡åº¦: 0.0-1.0\n\nè®ºç‚¹2: "<ä¸€å¥è¯æ ¸å¿ƒè§‚ç‚¹>" (å¯é€‰)\nè®ºæ®: ["<æ”¯æ’‘è®ºæ®1>", "<æ”¯æ’‘è®ºæ®2>"]\nç«‹åœº: "èµæˆ|åå¯¹|å¼ƒæƒ"\nç½®ä¿¡åº¦: 0.0-1.0\n'

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=input_text),
    ]

    # æ”¶é›†å®Œæ•´å“åº”
    full_response = ""
    input_tokens = 0
    output_tokens = 0

    # æµå¼è°ƒç”¨
    for chunk in client.stream(messages):
        chunk_text = _format_chunk_content(chunk)
        if chunk_text:
            full_response += chunk_text
            # è°ƒç”¨å›è°ƒå‡½æ•°ä¼ é€’ç‰‡æ®µ
            if stream_callback:
                stream_callback(chunk_text)

    # è·å–çœŸå®çš„ token ä½¿ç”¨é‡
    input_tokens, output_tokens = _get_token_usage(
        client, system_prompt, input_text, full_response
    )

    total_tokens = input_tokens + output_tokens
    remaining = max(0, model_max_tokens - total_tokens)

    # è§¦å‘ token å›è°ƒ
    if token_callback:
        token_callback({
            "role": role,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": total_tokens,
            "remaining": remaining,
            "max_tokens": model_max_tokens,
            "usage_percent": (total_tokens / model_max_tokens * 100) if model_max_tokens > 0 else 0,
        })

    return full_response


def run_debate(
    content: str,
    current_round: int,
    roles: Optional[list[str]] = None,
    stream_callback: Optional[Callable[[str, str], None]] = None,
    token_callback: Optional[Callable[[dict], None]] = None,
) -> list[dict]:
    """é¡ºåºè¾©è®ºæ¨¡å¼ - æ¯ä¸ªè§’è‰²ä¾æ¬¡å‘è¨€ï¼Œèƒ½çœ‹åˆ°ä¹‹å‰æ‰€æœ‰è§’è‰²çš„è§‚ç‚¹

    Args:
        content: å¾…è¯„å®¡å†…å®¹ï¼ˆRFCæˆ–åˆ›æ–°æƒ³æ³•ï¼‰
        current_round: å½“å‰è½®æ¬¡
        roles: è§’è‰²åˆ—è¡¨ï¼Œé»˜è®¤ä»é…ç½®è¯»å–è¯„å®¡è€…è§’è‰²
        stream_callback: å¯é€‰çš„æµå¼å›è°ƒï¼Œå‚æ•°ä¸º (role, chunk_content)
        token_callback: å¯é€‰çš„tokenä½¿ç”¨é‡å›è°ƒï¼Œæ¥æ”¶ {role, input_tokens, output_tokens, total_tokens, remaining, max_tokens, usage_percent}

    Returns:
        è¯„å®¡ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«: {role, content, vote}
    """
    if roles is None:
        roles = get_reviewer_roles()

    results = []

    for role in roles:
        # åˆ›å»ºæµå¼å›è°ƒåŒ…è£…å™¨
        def make_callback(rl: str) -> Callable[[str], None]:
            def callback(chunk: str) -> None:
                if stream_callback:
                    stream_callback(rl, chunk)

            return callback

        role_callback = make_callback(role)

        # åˆ›å»º token å›è°ƒåŒ…è£…å™¨
        def make_token_callback(rl: str) -> Callable[[dict], None]:
            def callback(token_data: dict) -> None:
                token_data["role"] = rl
                if token_callback:
                    token_callback(token_data)

            return callback

        role_token_callback = make_token_callback(role)

        try:
            # ä¼ å…¥ä¹‹å‰çš„ç»“æœï¼Œè®©å½“å‰è§’è‰²å¯ä»¥çœ‹åˆ°è¾©è®ºå†å²
            response_text = _run_role_review_stream(
                role=role,
                content=content,
                current_round=current_round,
                stream_callback=role_callback,
                previous_results=results,  # ä¼ é€’å†å²ç»“æœ
                token_callback=role_token_callback,
            )

            # è§£ææŠ•ç¥¨ç»“æœ
            vote = _parse_vote(response_text)

            results.append({
                "role": role,
                "content": response_text,
                "vote": vote,
            })

        except Exception as e:
            results.append({
                "role": role,
                "content": f"è¯„å®¡å¤±è´¥ï¼š{str(e)}",
                "vote": None,
            })

    return results


# ä¿æŒå‘åå…¼å®¹çš„åˆ«å
run_parallel_review = run_debate


def analyze_votes(results: list[dict]) -> dict:
    """åˆ†ææŠ•ç¥¨ç»“æœ

    Args:
        results: run_parallel_review çš„è¿”å›ç»“æœ

    Returns:
        æŠ•ç¥¨ç»Ÿè®¡: {yes, no, abstain, needs_human}
    """
    votes = [r["vote"] for r in results if r["vote"]]
    if not votes:
        return {"yes": 0, "no": 0, "abstain": 0, "needs_human": False}

    yes_count = votes.count("èµæˆ")
    no_count = votes.count("åå¯¹")
    abstain_count = votes.count("å¼ƒæƒ")

    # åå¯¹ç¥¨è¶…è¿‡30%è§†ä¸ºéœ€è¦äººç±»ä»‹å…¥
    total = len(votes)
    needs_human = (no_count / total) > 0.3

    return {
        "yes": yes_count,
        "no": no_count,
        "abstain": abstain_count,
        "needs_human": needs_human,
    }


def _parse_vote(text: Union[str, list]) -> Optional[str]:
    """ä»è¯„å®¡æ–‡æœ¬ä¸­è§£ææŠ•ç¥¨ç»“æœ

    æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
    1. æ—§æ ¼å¼: ç«‹åœº: "èµæˆ|åå¯¹|å¼ƒæƒ"
    2. æ–°æ ¼å¼: è®ºç‚¹åˆ—è¡¨ä¸­æ¯ä¸ªè®ºç‚¹éƒ½æœ‰è‡ªå·±çš„ç«‹åœº

    è¿”å›å€¼ï¼šå¦‚æœæœ‰å¤šä¸ªè®ºç‚¹ï¼Œè¿”å›å¤šæ•°ç«‹åœºï¼›å¦‚æœæ— æ³•è§£æï¼Œè¿”å›None
    """
    import re
    # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œå°è¯•æ‰¾åˆ°å­—ç¬¦ä¸²å…ƒç´ 
    if isinstance(text, list):
        text = str(text)

    # æŸ¥æ‰¾æ‰€æœ‰è®ºç‚¹ä¸­çš„ç«‹åœº
    all_votes = []

    # åŒ¹é…è®ºç‚¹1ã€è®ºç‚¹2ç­‰æ ¼å¼ä¸­çš„ç«‹åœº
    vote_patterns = [
        r"è®ºç‚¹\d+[:ï¼š].*?ç«‹åœº[:ï¼š]\s*[\"']?\s*(èµæˆ|åå¯¹|å¼ƒæƒ)",
        r"ç«‹åœº[:ï¼š]\s*[\"']?\s*(èµæˆ|åå¯¹|å¼ƒæƒ)",
        r"(èµæˆ|åå¯¹|å¼ƒæƒ)[,ï¼Œ]",
    ]

    for pattern in vote_patterns:
        matches = re.findall(pattern, text, re.IGNORECASE)
        for match in matches:
            vote = match.strip()
            # æ ‡å‡†åŒ–
            if vote in ["åŒæ„", "èµæˆ", "æ”¯æŒ"]:
                all_votes.append("èµæˆ")
            elif vote in ["åå¯¹", "ä¸æ”¯æŒ", "æ‹’ç»"]:
                all_votes.append("åå¯¹")
            elif vote in ["å¼ƒæƒ", "ä¸å‘è¡¨æ„è§"]:
                all_votes.append("å¼ƒæƒ")

    if not all_votes:
        return None

    # è¿”å›å¤šæ•°ç«‹åœº
    from collections import Counter

    vote_counts = Counter(all_votes)
    return vote_counts.most_common(1)[0][0]


def check_approval(
    vote_result: dict,
    max_rounds: int,
    current_round: int,
    yes_votes_needed: int = 2,
    no_votes_limit: int = 2,
    require_yes_over_no: bool = True,
) -> dict:
    """æ£€æŸ¥æ˜¯å¦é€šè¿‡å®¡æ ¸

    Args:
        vote_result: analyze_votes çš„è¿”å›ç»“æœ
        max_rounds: æœ€å¤§è½®æ¬¡
        current_round: å½“å‰è½®æ¬¡
        yes_votes_needed: éœ€è¦çš„æœ€å°‘èµæˆç¥¨
        no_votes_limit: åå¯¹ç¥¨ä¸Šé™
        require_yes_over_no: æ˜¯å¦è¦æ±‚èµæˆç¥¨å¤šäºåå¯¹ç¥¨

    Returns:
        {approved, finished, reason}
    """
    yes = vote_result["yes"]
    no = vote_result["no"]

    # æ£€æŸ¥èµæˆç¥¨æ˜¯å¦è¶³å¤Ÿ
    if yes >= yes_votes_needed:
        if require_yes_over_no:
            if yes > no:
                return {"approved": True, "finished": True, "reason": "é€šè¿‡å®¡æ ¸"}
        else:
            return {"approved": True, "finished": True, "reason": "é€šè¿‡å®¡æ ¸"}

    # æ£€æŸ¥åå¯¹ç¥¨æ˜¯å¦è¶…è¿‡ä¸Šé™
    if no >= no_votes_limit:
        return {"approved": False, "finished": True, "reason": "åå¯¹ç¥¨è¿‡å¤š"}

    # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è½®æ¬¡
    if current_round >= max_rounds:
        return {"approved": False, "finished": True, "reason": "è¾¾åˆ°æœ€å¤§è½®æ¬¡"}

    return {"approved": False, "finished": False, "reason": "ç»§ç»­è¾©è®º"}


# === è§‚ç‚¹æ± ç›¸å…³å‡½æ•° ===

def _normalize_stance(stance: str) -> str:
    """æ ‡å‡†åŒ–ç«‹åœºè¡¨è¾¾"""
    stance = stance.strip()
    if stance in ["åŒæ„", "èµæˆ", "æ”¯æŒ", "yes", "Yes", "YES"]:
        return "èµæˆ"
    elif stance in ["åå¯¹", "ä¸æ”¯æŒ", "æ‹’ç»", "no", "No", "NO"]:
        return "åå¯¹"
    elif stance in ["å¼ƒæƒ", "ä¸å‘è¡¨æ„è§", "ä¸­ç«‹", "abstain", "Abstain"]:
        return "å¼ƒæƒ"
    return stance


def parse_viewpoints(text: str) -> List[dict]:
    """ä»è¯„å®¡æ–‡æœ¬ä¸­è§£æè®ºç‚¹ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰

    Args:
        text: è¯„å®¡æ–‡æœ¬å†…å®¹

    Returns:
        è®ºç‚¹åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {content, evidence, stance, is_new}
    """
    import re

    viewpoints = []

    # è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥å†…å®¹æ˜¯å¦å·²å­˜åœ¨
    def content_exists(content: str) -> bool:
        return any(vp["content"] == content for vp in viewpoints)

    # æ¨¡å¼1ï¼šæ˜ç¡®æ ‡æ³¨ä¸º"æ–°è§‚ç‚¹"çš„è®ºç‚¹
    # æ ¼å¼ï¼šè®ºç‚¹1ï¼ˆæ–°è§‚ç‚¹ï¼‰: "..." è®ºæ®: [...] ç«‹åœº: ...
    new_pattern = r"è®ºç‚¹\s*\d+\s*[\ï¼ˆ\(]?\s*æ–°\s*è§‚ç‚¹\s*[\ï¼‰\)]?[:ï¼š]\s*[\"']([^\"']+)[\"']\s*\nè®ºæ®:\s*\[([^\]]+)\]\s*\nç«‹åœº:\s*[\"']?([^\"'\n]+)[\"']?"

    for match in re.finditer(new_pattern, text, re.DOTALL):
        content = match.group(1).strip()
        evidence_str = match.group(2)
        stance = match.group(3).strip()

        if content_exists(content):
            continue

        evidence = [e.strip().strip('"').strip("'") for e in evidence_str.split(',')]

        viewpoints.append({
            "content": content,
            "evidence": evidence,
            "stance": _normalize_stance(stance),
            "is_new": True,
        })

    # æ¨¡å¼2ï¼šè®ºç‚¹åˆ—è¡¨æ ¼å¼ï¼ˆé€šç”¨æ ¼å¼ï¼‰
    # æ ¼å¼ï¼šè®ºç‚¹1: "..." è®ºæ®: [...] ç«‹åœº: ...
    general_pattern = r"è®ºç‚¹\s*\d+[:ï¼š]\s*[\"']([^\"']+)[\"']\s*\n?\s*è®ºæ®:\s*\[([^\]]+)\]\s*\n?\s*ç«‹åœº:\s*[\"']?([^\"'\n]+)[\"']?"

    for match in re.finditer(general_pattern, text, re.DOTALL | re.IGNORECASE):
        content = match.group(1).strip()
        evidence_str = match.group(2)
        stance = match.group(3).strip()

        if content_exists(content):
            continue

        evidence = [e.strip().strip('"').strip("'") for e in evidence_str.split(',')]

        viewpoints.append({
            "content": content,
            "evidence": evidence,
            "stance": _normalize_stance(stance),
            "is_new": False,
        })

    # æ¨¡å¼3ï¼šç®€åŒ–æ ¼å¼ï¼ˆè®ºç‚¹: å†…å®¹ï¼Œè®ºæ®: [...]ï¼Œç«‹åœº: ...ï¼‰
    simple_pattern = r"è®ºç‚¹[:ï¼š]\s*([^\n]+)\n?\s*è®ºæ®[:ï¼š]\s*\[([^\]]+)\]\s*\n?\s*ç«‹åœº[:ï¼š]\s*([^\n]+)"

    for match in re.finditer(simple_pattern, text, re.DOTALL | re.IGNORECASE):
        content = match.group(1).strip()
        evidence_str = match.group(2)
        stance = match.group(3).strip()

        if content_exists(content):
            continue

        evidence = [e.strip().strip('"').strip("'") for e in evidence_str.split(',')]

        viewpoints.append({
            "content": content,
            "evidence": evidence,
            "stance": _normalize_stance(stance),
            "is_new": False,
        })

    return viewpoints


def build_viewpoint_pool_context(viewpoint_pool: List[Viewpoint]) -> str:
    """æ„å»ºè§‚ç‚¹æ± ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²

    Args:
        viewpoint_pool: å½“å‰è§‚ç‚¹æ± 

    Returns:
        ç”¨äº LLM æç¤ºçš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
    """
    if not viewpoint_pool:
        return "å½“å‰è§‚ç‚¹æ± ä¸ºç©ºï¼Œå¯ä»¥æå‡ºæ–°çš„æ ¸å¿ƒè§‚ç‚¹ã€‚"

    context_lines = ["=== å½“å‰æ´»è·ƒè§‚ç‚¹æ± ï¼ˆæœ€å¤š3ä¸ªï¼Œå¿…é¡»é€ä¸€å›åº”ï¼‰==="]

    for i, vp in enumerate(viewpoint_pool, 1):
        status_icon = "ğŸ”´" if vp.status == ViewpointStatus.ACTIVE else "ğŸŸ¢"
        votes_info = f"ğŸ‘{vp.vote_count.get('èµæˆ', 0)} ğŸ‘{vp.vote_count.get('åå¯¹', 0)}"

        context_lines.append(f"\n{status_icon} è§‚ç‚¹{i} [{vp.id}]: {vp.content}")
        context_lines.append(f"   æå‡ºè€…: {vp.proposer} | æŠ•ç¥¨: {votes_info}")
        context_lines.append(f"   è®ºæ®: {'; '.join(vp.evidence[:2])}")

    context_lines.append("\n=== è®¨è®ºè§„åˆ™ ===")
    context_lines.append("1. ä½ å¿…é¡»å…ˆå›åº”è§‚ç‚¹æ± ä¸­çš„æ‰€æœ‰è§‚ç‚¹ï¼ˆæ¯ä¸ªè§‚ç‚¹è‡³å°‘ä¸€æ¡æ„è§ï¼‰")
    context_lines.append("2. åªèƒ½æå‡ºæœ€å¤š1ä¸ªæ–°è§‚ç‚¹ï¼ˆå¦‚æœè§‚ç‚¹æ± æœªæ»¡ï¼‰")
    context_lines.append("3. å›åº”ç°æœ‰è§‚ç‚¹æ—¶ï¼Œè¯´æ˜æ”¯æŒã€åå¯¹æˆ–è¡¥å……ç†ç”±")

    return "\n".join(context_lines)


def can_propose_new_viewpoint(viewpoint_pool: List[Viewpoint], pool_limit: int = 3) -> bool:
    """æ£€æŸ¥æ˜¯å¦è¿˜èƒ½æå‡ºæ–°è§‚ç‚¹"""
    return len(viewpoint_pool) < pool_limit


def run_review_with_viewpoint_pool(
    role: str,
    content: str,
    current_round: int,
    viewpoint_pool: List[Viewpoint],
    stream_callback: Optional[Callable[[str], None]] = None,
    previous_results: Optional[list[dict]] = None,
    token_callback: Optional[Callable[[dict], None]] = None,
) -> dict:
    """å¸¦è§‚ç‚¹æ± ä¸Šä¸‹æ–‡çš„è§’è‰²è¯„å®¡

    Args:
        role: è§’è‰²åç§°
        content: å¾…è¯„å®¡å†…å®¹
        current_round: å½“å‰è½®æ¬¡
        viewpoint_pool: å½“å‰è§‚ç‚¹æ± 
        stream_callback: æµå¼å›è°ƒå‡½æ•°
        previous_results: ä¹‹å‰è§’è‰²çš„è¯„å®¡ç»“æœ
        token_callback: Tokenä½¿ç”¨é‡å›è°ƒ

    Returns:
        è¯„å®¡ç»“æœ {role, content, vote, new_viewpoints}
    """
    import re

    system_prompt = get_role_prompt(role)
    client = _get_client_for_role(role)

    config = get_role_llm_config(role)
    model_max_tokens = getattr(config, 'max_tokens', 128000) or 128000

    # æ„å»ºè¾“å…¥æ–‡æœ¬
    input_text = f"è¯·è¯„å®¡ä»¥ä¸‹å†…å®¹ï¼ˆè½®æ¬¡ï¼š{current_round}ï¼‰ï¼š\n\n{content}\n"

    # æ·»åŠ è§‚ç‚¹æ± ä¸Šä¸‹æ–‡
    input_text += "\n" + build_viewpoint_pool_context(viewpoint_pool)

    # å¦‚æœæœ‰ä¹‹å‰çš„è¯„å®¡ç»“æœï¼Œæ·»åŠ è¾©è®ºå†å²
    if previous_results:
        input_text += "\n=== ä¹‹å‰è§’è‰²çš„è§‚ç‚¹ ===\n"
        for result in previous_results:
            role_name = result.get("role", "æœªçŸ¥")
            role_content = result.get("content", "")
            role_vote = result.get("vote", "")

            # æå–è®ºç‚¹åˆ—è¡¨
            points = []
            point_pattern = r"è®ºç‚¹\d+[:ï¼š]([^\n]+)"
            for match in re.finditer(point_pattern, role_content):
                points.append(match.group(1).strip())

            input_text += f"\nã€{role_name}ã€‘ç«‹åœº: {role_vote or 'æœªçŸ¥'}\n"
            if points:
                for i, p in enumerate(points[:3], 1):
                    input_text += f"  è®ºç‚¹{i}: {p[:100]}...\n"
            else:
                input_text += f"  è§‚ç‚¹: {role_content[:200]}...\n"

    # æ£€æŸ¥æ˜¯å¦å¯ä»¥æå‡ºæ–°è§‚ç‚¹
    can_add = can_propose_new_viewpoint(viewpoint_pool)

    input_text += '''
=== è¾“å‡ºæ ¼å¼ï¼ˆå¿…é¡»ä¸¥æ ¼éµå¾ªï¼‰ ===

## è‚¯å®šç‚¹
- [å¦‚æœæœ‰å€¼å¾—è‚¯å®šçš„è®¾è®¡ï¼Œå†™åœ¨è¿™é‡Œ]

## æ–°è§‚ç‚¹ï¼ˆåªæœ‰æ˜ç¡®åŒæ„/åå¯¹ç°æœ‰è§‚ç‚¹åï¼Œæ‰èƒ½æå‡ºæ–°è§‚ç‚¹ï¼‰
è®ºç‚¹1ï¼ˆæ–°è§‚ç‚¹ï¼‰: "<ä¸€å¥è¯æ ¸å¿ƒè§‚ç‚¹>"
è®ºæ®: ["<æ”¯æ’‘è®ºæ®1>", "<æ”¯æ’‘è®ºæ®2>"]
ç«‹åœº: "èµæˆ|åå¯¹|å¼ƒæƒ"

## å¯¹ç°æœ‰è§‚ç‚¹çš„å›åº”
å›åº”1: "é’ˆå¯¹è§‚ç‚¹Xçš„IDï¼Œä½ çš„çœ‹æ³•"
ç«‹åœº: "èµæˆ|åå¯¹|å¼ƒæƒ"
'''

    if not can_add:
        input_text += '''
ï¼ˆæ³¨æ„ï¼šè§‚ç‚¹æ± å·²æ»¡ï¼Œä¸èƒ½æå‡ºæ–°è§‚ç‚¹ï¼Œåªèƒ½å›åº”ç°æœ‰è§‚ç‚¹ï¼‰
'''

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=input_text),
    ]

    # æ”¶é›†å®Œæ•´å“åº”
    full_response = ""
    input_tokens = 0
    output_tokens = 0

    # æµå¼è°ƒç”¨
    for chunk in client.stream(messages):
        chunk_text = _format_chunk_content(chunk)
        if chunk_text:
            full_response += chunk_text
            if stream_callback:
                stream_callback(chunk_text)

    # è·å–çœŸå®çš„ token ä½¿ç”¨é‡
    input_tokens, output_tokens = _get_token_usage(
        client, system_prompt, input_text, full_response
    )

    total_tokens = input_tokens + output_tokens
    remaining = max(0, model_max_tokens - total_tokens)

    if token_callback:
        token_callback({
            "role": role,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": total_tokens,
            "remaining": remaining,
            "max_tokens": model_max_tokens,
            "usage_percent": (total_tokens / model_max_tokens * 100) if model_max_tokens > 0 else 0,
        })

    # è§£ææŠ•ç¥¨ç»“æœ
    vote = _parse_vote(full_response)

    # è§£ææ–°è§‚ç‚¹
    new_viewpoints = parse_viewpoints(full_response)

    return {
        "role": role,
        "content": full_response,
        "vote": vote,
        "new_viewpoints": new_viewpoints,
    }


# === å¤šæ®µæ€è€ƒ Agentï¼ˆReAct æ¨¡å¼ï¼‰===

def run_review_with_tools(
    role: str,
    content: str,
    current_round: int,
    viewpoint_pool: List[Viewpoint],
    stream_callback: Optional[Callable[[str], None]] = None,
    previous_results: Optional[list[dict]] = None,
    token_callback: Optional[Callable[[dict], None]] = None,
    max_iterations: int = 10,
) -> dict:
    """å¸¦å·¥å…·è°ƒç”¨çš„å¤šæ®µæ€è€ƒè¯„å®¡ï¼ˆReAct æ¨¡å¼ï¼‰

    å®ç°çœŸæ­£çš„ AI æ€è€ƒè¿‡ç¨‹ï¼š
    1. æ€è€ƒï¼šåˆ†æé—®é¢˜ï¼Œå†³å®šæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·
    2. è¡ŒåŠ¨ï¼šè°ƒç”¨å·¥å…·è·å–ä¿¡æ¯ï¼ˆæœç´¢ä»£ç ã€è¯»å–æ–‡ä»¶ç­‰ï¼‰
    3. è§‚å¯Ÿï¼šè·å–å·¥å…·è¿”å›ç»“æœ
    4. é‡å¤ï¼šç›´åˆ°å¾—åˆ°æœ€ç»ˆç­”æ¡ˆ

    Args:
        role: è§’è‰²åç§°
        content: å¾…è¯„å®¡å†…å®¹
        current_round: å½“å‰è½®æ¬¡
        viewpoint_pool: å½“å‰è§‚ç‚¹æ± 
        stream_callback: æµå¼å›è°ƒå‡½æ•°
        previous_results: ä¹‹å‰è§’è‰²çš„è¯„å®¡ç»“æœ
        token_callback: Tokenä½¿ç”¨é‡å›è°ƒ
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰

    Returns:
        è¯„å®¡ç»“æœ {role, content, vote, new_viewpoints, tool_calls}
    """
    try:
        from langgraph.prebuilt import create_react_agent
        from langchain_core.messages import HumanMessage
    except ImportError:
        # å¦‚æœ langgraph.prebuilt ä¸å¯ç”¨ï¼Œå›é€€åˆ°æ™®é€šæ¨¡å¼
        return run_review_with_viewpoint_pool(
            role=role,
            content=content,
            current_round=current_round,
            viewpoint_pool=viewpoint_pool,
            stream_callback=stream_callback,
            previous_results=previous_results,
            token_callback=token_callback,
        )

    system_prompt = get_role_prompt(role)
    client = _get_client_for_role(role)

    config = get_role_llm_config(role)
    model_max_tokens = getattr(config, 'max_tokens', 128000) or 128000

    # æ„å»ºè¾“å…¥æ–‡æœ¬
    input_text = f"è¯·è¯„å®¡ä»¥ä¸‹å†…å®¹ï¼ˆè½®æ¬¡ï¼š{current_round}ï¼‰ï¼š\n\n{content}\n"

    # æ·»åŠ è§‚ç‚¹æ± ä¸Šä¸‹æ–‡
    input_text += "\n" + build_viewpoint_pool_context(viewpoint_pool)

    # å¦‚æœæœ‰ä¹‹å‰çš„è¯„å®¡ç»“æœï¼Œæ·»åŠ è¾©è®ºå†å²
    if previous_results:
        import re
        input_text += "\n=== ä¹‹å‰è§’è‰²çš„è§‚ç‚¹ ===\n"
        for result in previous_results:
            role_name = result.get("role", "æœªçŸ¥")
            role_content = result.get("content", "")
            role_vote = result.get("vote", "")

            points = []
            point_pattern = r"è®ºç‚¹\d+[:ï¼š]([^\n]+)"
            for match in re.finditer(point_pattern, role_content):
                points.append(match.group(1).strip())

            input_text += f"\nã€{role_name}ã€‘ç«‹åœº: {role_vote or 'æœªçŸ¥'}\n"
            if points:
                for i, p in enumerate(points[:3], 1):
                    input_text += f"  è®ºç‚¹{i}: {p[:100]}...\n"
            else:
                input_text += f"  è§‚ç‚¹: {role_content[:200]}...\n"

    # æ·»åŠ å·¥å…·ä½¿ç”¨è¯´æ˜
    input_text += """
=== å¯ç”¨å·¥å…· ===
ä½ å¯ä»¥è°ƒç”¨ä»¥ä¸‹å·¥å…·æ¥è·å–ä¿¡æ¯æˆ–ç®¡ç†è§‚ç‚¹ï¼š

ã€ä¿¡æ¯è·å–å·¥å…·ã€‘
- file_read: è¯»å–æ–‡ä»¶å†…å®¹ã€‚å‚æ•°: file_path(æ–‡ä»¶è·¯å¾„)
- file_search: é€’å½’æŸ¥æ‰¾æ–‡ä»¶ã€‚å‚æ•°: start_dir(èµ·å§‹ç›®å½•), pattern(æ–‡ä»¶åŒ¹é…æ¨¡å¼, å¦‚ "*.py")
- code_search: åœ¨ä»£ç ä¸­æœç´¢æ­£åˆ™è¡¨è¾¾å¼ã€‚å‚æ•°: pattern(æ­£åˆ™è¡¨è¾¾å¼), file_pattern(æ–‡ä»¶åŒ¹é…æ¨¡å¼)
- list_dir: åˆ—å‡ºç›®å½•å†…å®¹ã€‚å‚æ•°: dir_path(ç›®å½•è·¯å¾„)

ã€è§‚ç‚¹ç®¡ç†å·¥å…·ã€‘ï¼ˆéå¸¸é‡è¦ï¼Œå¿…é¡»ä½¿ç”¨ï¼‰
- propose_viewpoint: æå‡ºæ–°è§‚ç‚¹åˆ°è§‚ç‚¹æ± ã€‚
  å‚æ•°:
    - content: è§‚ç‚¹å†…å®¹ï¼ˆä¸€å¥è¯æ¦‚æ‹¬æ ¸å¿ƒé—®é¢˜ï¼‰
    - evidence: æ”¯æ’‘è®ºæ®åˆ—è¡¨ï¼ˆJSONæ•°ç»„æ ¼å¼ï¼Œå¦‚ ["è®ºæ®1", "è®ºæ®2"]ï¼‰
    - stance: ä½ çš„ç«‹åœºï¼ˆå¿…é¡»æ˜¯ "èµæˆ"ã€"åå¯¹" æˆ– "å¼ƒæƒ" ä¹‹ä¸€ï¼‰
  ç¤ºä¾‹: propose_viewpoint({"content": "APIè®¾è®¡è¿‡äºå¤æ‚", "evidence": ["æ¥å£å‚æ•°è¿‡å¤š", "ç¼ºä¹é»˜è®¤å€¼"], "stance": "åå¯¹"})

- respond_to_viewpoint: å›åº”è§‚ç‚¹æ± ä¸­çš„å·²æœ‰è§‚ç‚¹ã€‚
  å‚æ•°:
    - viewpoint_id: è¦å›åº”çš„è§‚ç‚¹ID
    - response: ä½ çš„å›åº”å†…å®¹
    - stance: ä½ å¯¹è¯¥è§‚ç‚¹çš„ç«‹åœºï¼ˆ"èµæˆ"ã€"åå¯¹" æˆ– "å¼ƒæƒ"ï¼‰
  ç¤ºä¾‹: respond_to_viewpoint({"viewpoint_id": "VP-001", "response": "åŒæ„æ­¤è§‚ç‚¹ï¼Œè¡¥å……å¦‚ä¸‹...", "stance": "èµæˆ"})

=== æ ¸å¿ƒè§„åˆ™ï¼ˆå¿…é¡»éµå®ˆï¼‰ ===
ã€é‡è¦ã€‘å¦‚æœä½ å‘ç°äº†æ–°çš„è®¾è®¡é—®é¢˜æˆ–å…³æ³¨ç‚¹ï¼Œå¿…é¡»é€šè¿‡è°ƒç”¨ propose_viewpoint å·¥å…·æ¥æ·»åŠ è§‚ç‚¹ï¼Œè€Œä¸æ˜¯åœ¨å›å¤æ–‡æœ¬ä¸­æåŠï¼
ã€é‡è¦ã€‘å¦‚æœä½ æƒ³å¯¹ç°æœ‰è§‚ç‚¹è¡¨è¾¾ç«‹åœºï¼Œå¿…é¡»è°ƒç”¨ respond_to_viewpoint å·¥å…·ï¼
ã€é‡è¦ã€‘åªæœ‰åœ¨è§‚ç‚¹æ± å·²æ»¡ï¼ˆå·²æœ‰3ä¸ªæ´»è·ƒè§‚ç‚¹ï¼‰æ—¶ï¼Œæ‰ä¸èƒ½æå‡ºæ–°è§‚ç‚¹ï¼

=== æ€è€ƒæµç¨‹ ===
1. å…ˆæ€è€ƒæ˜¯å¦éœ€è¦è°ƒç”¨å·¥å…·è·å–æ›´å¤šä¿¡æ¯
2. å¦‚æœéœ€è¦ï¼Œè°ƒç”¨ç›¸å…³å·¥å…·
3. å¦‚æœè§‚ç‚¹æ± æœªæ»¡ä¸”å‘ç°æ–°é—®é¢˜ï¼Œè°ƒç”¨ propose_viewpoint æå‡ºæ–°è§‚ç‚¹ï¼ˆè¿™æ˜¯å”¯ä¸€æ·»åŠ è§‚ç‚¹çš„æ–¹å¼ï¼ï¼‰
4. å¯¹è§‚ç‚¹æ± ä¸­çš„ç°æœ‰è§‚ç‚¹ï¼Œè°ƒç”¨ respond_to_viewpoint è¡¨è¾¾ä½ çš„ç«‹åœº
5. æ ¹æ®å·¥å…·è¿”å›çš„ç»“æœç»§ç»­æ€è€ƒ
6. æœ€ç»ˆç»™å‡ºè¯„å®¡ç»“è®º

=== è¾“å‡ºæ ¼å¼ ===
## è‚¯å®šç‚¹
- [å¦‚æœæœ‰å€¼å¾—è‚¯å®šçš„è®¾è®¡]

## æ€»ç»“
[å¯¹ä½ çš„æ•´ä½“è¯„å®¡ç»“æœ]
"""

    # è·å–å·¥å…·åˆ—è¡¨
    tools = get_all_tools()

    # åˆ›å»º ReAct Agent
    try:
        # LangGraph æ–°ç‰ˆæœ¬ä½¿ç”¨ prompt å‚æ•°ï¼ˆæ—§ç‰ˆæœ¬æ˜¯ state_modifierï¼‰
        agent = create_react_agent(client, tools, prompt=system_prompt)
    except Exception as e:
        # å¦‚æœåˆ›å»ºå¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šæ¨¡å¼
        if stream_callback:
            stream_callback(f"\n[å›é€€åˆ°æ™®é€šæ¨¡å¼: {str(e)[:100]}]\n")
        return run_review_with_viewpoint_pool(
            role=role,
            content=content,
            current_round=current_round,
            viewpoint_pool=viewpoint_pool,
            stream_callback=stream_callback,
            previous_results=previous_results,
            token_callback=token_callback,
        )

    # æ”¶é›†å®Œæ•´å“åº”
    full_response = ""
    tool_calls = []
    input_tokens = 0
    output_tokens = 0
    thought_cycle_count = 0  # è®°å½•å®é™…æ€è€ƒè½®æ¬¡ï¼ˆæ¯æ¬¡AIå†³å®šè°ƒç”¨å·¥å…·ç®—ä¸€è½®ï¼‰
    max_thought_cycles = min(max_iterations, 15)  # æœ€å¤š15è½®æ€è€ƒï¼Œä½¿ç”¨ä¼ å…¥çš„max_iterations
    force_stop = False  # å¼ºåˆ¶åœæ­¢æ ‡å¿—

    try:
        # è¿è¡Œ Agentï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
        for event in agent.stream(
            {"messages": [HumanMessage(content=input_text)]},
            {"recursion_limit": max_thought_cycles * 3},  # æ¯ä¸ªæ€è€ƒè½®æ¬¡å¯èƒ½äº§ç”Ÿå¤šä¸ªäº‹ä»¶
        ):
            if force_stop:
                break

            try:
                # å¤„ç†äº‹ä»¶ï¼Œæå–æ¶ˆæ¯å†…å®¹
                if "messages" in event:
                    for message in event["messages"]:
                        # æ£€æµ‹ AI æ˜¯å¦å¼€å§‹æ–°çš„ä¸€è½®æ€è€ƒï¼ˆå†³å®šè°ƒç”¨å·¥å…·ï¼‰
                        if hasattr(message, 'tool_calls') and message.tool_calls:
                            thought_cycle_count += 1

                            # é˜²æ­¢æ— é™å¾ªç¯å®‰å…¨æ£€æŸ¥
                            if thought_cycle_count > max_thought_cycles:
                                if stream_callback:
                                    stream_callback(f"\n[è­¦å‘Š: è¾¾åˆ°æœ€å¤§æ€è€ƒè½®æ¬¡ {max_thought_cycles}ï¼Œå¼ºåˆ¶ç»“æŸ]\n")
                                force_stop = True
                                break

                        # AI çš„æ€è€ƒå’Œå›å¤
                        if hasattr(message, 'content') and message.content:
                            content_str = str(message.content)
                            # è¿‡æ»¤æ‰çº¯å·¥å…·è°ƒç”¨å®šä¹‰ï¼Œä¿ç•™æ€è€ƒå†…å®¹å’Œæœ€ç»ˆå›å¤
                            # åªè¿‡æ»¤ä»¥ JSON æ ¼å¼çš„å·¥å…·è°ƒç”¨å—
                            lines = content_str.split('\n')
                            filtered_lines = []
                            skip_next = False
                            for i, line in enumerate(lines):
                                if skip_next:
                                    skip_next = False
                                    continue
                                # è·³è¿‡çº¯ JSON å·¥å…·è°ƒç”¨å—
                                if line.strip().startswith('"name":') or line.strip().startswith('"args"'):
                                    # æ£€æŸ¥æ˜¯å¦æ˜¯å·¥å…·è°ƒç”¨å®šä¹‰çš„ä¸€éƒ¨åˆ†
                                    if i > 0 and ('tool_calls' in lines[i-1] or 'function' in lines[i-1].lower()):
                                        skip_next = True
                                        continue
                                filtered_lines.append(line)

                            clean_content = '\n'.join(filtered_lines).strip()
                            if clean_content:
                                full_response += clean_content + "\n"
                                if stream_callback:
                                    stream_callback(clean_content)

                        # å·¥å…·è°ƒç”¨è®°å½•
                        if hasattr(message, 'tool_calls') and message.tool_calls:
                            for tc in message.tool_calls:
                                tool_calls.append({
                                    "tool": tc.get("name", "unknown"),
                                    "arguments": tc.get("args", {}),
                                })

                # å¤„ç†å·¥å…·ç»“æœï¼ˆå¸¦é”™è¯¯æ¢å¤ï¼‰
                if "tool" in event:
                    tool_result = event["tool"]
                    try:
                        if hasattr(tool_result, 'content'):
                            result_content = str(tool_result.content)
                            # å°†å·¥å…·ç»“æœæ·»åŠ åˆ°å“åº”ä¸­
                            if result_content and result_content.strip():
                                tool_result_text = f"\n[å·¥å…·ç»“æœ: {result_content[:200]}]\n"
                                full_response += tool_result_text
                                if stream_callback:
                                    stream_callback(tool_result_text)
                            if tool_calls:
                                tool_calls[-1]["result"] = result_content[:500] if result_content else ""
                    except Exception as tool_err:
                        # å•ä¸ªå·¥å…·è°ƒç”¨å¤±è´¥ä¸å½±å“æ•´ä½“
                        error_text = f"\n[å·¥å…·æ‰§è¡Œé”™è¯¯: {str(tool_err)[:100]}]\n"
                        full_response += error_text
                        if stream_callback:
                            stream_callback(error_text)
                        if tool_calls:
                            tool_calls[-1]["result"] = f"é”™è¯¯: {str(tool_err)[:200]}"

            except Exception as event_err:
                # å•ä¸ªäº‹ä»¶å¤„ç†å¤±è´¥ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                error_text = f"\n[äº‹ä»¶å¤„ç†é”™è¯¯: {str(event_err)[:100]}]\n"
                full_response += error_text
                if stream_callback:
                    stream_callback(error_text)
                continue

        # è·å–çœŸå®çš„ token ä½¿ç”¨é‡
        input_tokens, output_tokens = _get_token_usage(
            client, system_prompt, input_text, full_response
        )

        total_tokens = input_tokens + output_tokens
        remaining = max(0, model_max_tokens - total_tokens)

        if token_callback:
            token_callback({
                "role": role,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": total_tokens,
                "remaining": remaining,
                "max_tokens": model_max_tokens,
                "usage_percent": (total_tokens / model_max_tokens * 100) if model_max_tokens > 0 else 0,
            })

    except Exception as outer_err:
        # å¤–éƒ¨å¼‚å¸¸å¤„ç†ï¼šä¿ç•™å·²æ”¶é›†çš„å“åº”ï¼Œè€Œä¸æ˜¯å®Œå…¨å›é€€
        error_msg = f"\n[è¯„å®¡è¿‡ç¨‹å‡ºé”™: {str(outer_err)[:100]}]\n"
        full_response += error_msg
        if stream_callback:
            stream_callback(error_msg)

        # è·å–çœŸå®çš„ token ä½¿ç”¨é‡ï¼ˆå³ä½¿å‡ºé”™ä¹Ÿè¦æ›´æ–°ï¼‰
        input_tokens, output_tokens = _get_token_usage(
            client, system_prompt, input_text, full_response
        )
        total_tokens = input_tokens + output_tokens
        remaining = max(0, model_max_tokens - total_tokens)

        if token_callback:
            token_callback({
                "role": role,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
                "total_tokens": total_tokens,
                "remaining": remaining,
                "max_tokens": model_max_tokens,
                "usage_percent": (total_tokens / model_max_tokens * 100) if model_max_tokens > 0 else 0,
            })

    # è§£ææŠ•ç¥¨ç»“æœ
    vote = _parse_vote(full_response)

    # è§£ææ–°è§‚ç‚¹
    new_viewpoints = parse_viewpoints(full_response)

    # ä»å·¥å…·è°ƒç”¨ä¸­æå–é€šè¿‡ propose_viewpoint æ·»åŠ çš„è§‚ç‚¹
    from .tools import get_viewpoints_from_tool
    tool_viewpoints = get_viewpoints_from_tool()
    for vp in tool_viewpoints:
        # é¿å…é‡å¤æ·»åŠ 
        if not any(v["content"] == vp["content"] for v in new_viewpoints):
            new_viewpoints.append({
                "content": vp.get("content", ""),
                "evidence": vp.get("evidence", []),
                "stance": _normalize_stance(vp.get("stance", "å¼ƒæƒ")),
                "is_new": True,
            })

    return {
        "role": role,
        "content": full_response,
        "vote": vote,
        "new_viewpoints": new_viewpoints,
        "tool_calls": tool_calls,  # è®°å½•å·¥å…·è°ƒç”¨å†å²
    }
